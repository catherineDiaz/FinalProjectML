{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X51AfId2P7IX"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split as tts\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, Flatten\n",
    "from keras.utils import np_utils\n",
    "from random import randint\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "from matplotlib.pyplot import imshow\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-LZTd_MBP7Ib"
   },
   "outputs": [],
   "source": [
    "class DoodleCNN():\n",
    "    def __init__(self, data_dir, limit=None, split=0.05):\n",
    "        self.classes = {}\n",
    "        self.load(data_dir, split, limit)\n",
    "        self.buildModel()\n",
    "        \n",
    "    def load(self, data_dir, split=0.05, limit=None):\n",
    "        X = []\n",
    "        Y = []\n",
    "        label = -1\n",
    "        for file in os.listdir(data_dir):\n",
    "            if file.endswith(\".npy\"):\n",
    "                label += 1\n",
    "                contents = self.reshape(np.load(data_dir + file)[0:limit])\n",
    "                self.classes[label] = file[:-4]\n",
    "                X = X + contents\n",
    "                Y = Y + [label for _ in range(0, len(contents))]\n",
    "        X = self.normalize(X)\n",
    "        self.X_train, self.X_test, self.Y_train, self.Y_test = self.split(X, Y, split)\n",
    "        \n",
    "    def reshape(self, contents):\n",
    "        reshaped = []\n",
    "        for i in range(len(contents)):\n",
    "            image = np.reshape(contents[i], (28, 28, 1))\n",
    "            reshaped.append(image)\n",
    "        return reshaped\n",
    "        \n",
    "    def normalize(self, data):\n",
    "        return np.interp(data, [0, 255], [-1, 1])\n",
    "            \n",
    "    def split(self, X, Y, split=0.05):\n",
    "        X_train, X_test, Y_train, Y_test = tts(X, Y, test_size=split)\n",
    "        Y_train = np_utils.to_categorical(Y_train, len(self.classes))\n",
    "        Y_test = np_utils.to_categorical(Y_test, len(self.classes))\n",
    "        return X_train, X_test, Y_train, Y_test\n",
    "    \n",
    "    def buildModel(self):\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28,28,1), padding='same'))\n",
    "        self.model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "        self.model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        self.model.add(Dropout(0.25))\n",
    "        self.model.add(Flatten())\n",
    "        self.model.add(Dense(128, activation='relu'))\n",
    "        self.model.add(Dropout(0.5))\n",
    "        self.model.add(Dense(len(self.classes), activation='softmax'))\n",
    "        \n",
    "    def train(self, epochs=10):\n",
    "        self.model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        self.model.fit(np.array(self.X_train), np.array(self.Y_train), batch_size=32, epochs=epochs, shuffle=True)\n",
    "        \n",
    "    def test(self):\n",
    "        loss, acc = self.model.evaluate(self.X_test, self.Y_test, verbose=0)\n",
    "        print('\\nTesting acc: {}, loss: {}\\n'.format(acc, loss))\n",
    "        \n",
    "    def predict(self, arr):\n",
    "        return self.model.predict(arr)\n",
    "        \n",
    "    def randomPrediction(self):\n",
    "        index = random.randint(0,len(self.X_test) - 1)\n",
    "        \n",
    "        y_hat = self.predict(self.X_test)[index]\n",
    "        pred_class = self.classes[max(range(len(y_hat)), key=y_hat.__getitem__)]\n",
    "        \n",
    "        y = self.Y_test[index]\n",
    "        actual_class = self.classes[max(range(len(y)), key=y.__getitem__)]\n",
    "        \n",
    "        print(\"Predicition Probabilities:\", y_hat)\n",
    "        print(\"\\nPredicted Class: {}\\nActual Class: {}\".format(pred_class, actual_class))\n",
    "        \n",
    "        img = denormalize(self.X_test[index])\n",
    "        plot = plt.figure(figsize=(4,4))\n",
    "        plt.imshow(img.reshape(28,28))\n",
    "        plt.grid(False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "XFw4wk5wP7Id",
    "outputId": "3e9d8e3c-2b8b-42eb-98da-ee8b821217ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'blueberry', 1: 'apple', 2: 'blackberry', 3: 'pineapple', 4: 'pear', 5: 'watermelon', 6: 'banana', 7: 'strawberry', 8: 'grapes'}\n",
      "(42750, 28, 28, 1) (42750, 9) (2250, 28, 28, 1) (2250, 9)\n"
     ]
    }
   ],
   "source": [
    "nn = DoodleCNN(\"data/\", 5000, 0.05)\n",
    "print(nn.classes)\n",
    "print(np.shape(nn.X_train), np.shape(nn.Y_train), np.shape(nn.X_test), np.shape(nn.Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 408
    },
    "colab_type": "code",
    "id": "6gYld56HP7If",
    "outputId": "67426e23-d3be-4963-c198-e47da1d548d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_13 (Conv2D)           (None, 28, 28, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 28, 28, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 12544)             0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 128)               1605760   \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 9)                 1161      \n",
      "=================================================================\n",
      "Total params: 1,625,737\n",
      "Trainable params: 1,625,737\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "nn.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3801
    },
    "colab_type": "code",
    "id": "EOkStSOzP7Ih",
    "outputId": "388a679f-3d98-4a03-b43c-3950781cd393"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "42750/42750 [==============================] - 19s 451us/step - loss: 0.8456 - acc: 0.7208\n",
      "Epoch 2/100\n",
      "42750/42750 [==============================] - 18s 421us/step - loss: 0.6268 - acc: 0.7961\n",
      "Epoch 3/100\n",
      "42750/42750 [==============================] - 18s 420us/step - loss: 0.5574 - acc: 0.8166\n",
      "Epoch 4/100\n",
      "42750/42750 [==============================] - 18s 421us/step - loss: 0.5012 - acc: 0.8312\n",
      "Epoch 5/100\n",
      "42750/42750 [==============================] - 18s 416us/step - loss: 0.4539 - acc: 0.8467\n",
      "Epoch 6/100\n",
      "42750/42750 [==============================] - 18s 416us/step - loss: 0.4142 - acc: 0.8583\n",
      "Epoch 7/100\n",
      "42750/42750 [==============================] - 18s 417us/step - loss: 0.3825 - acc: 0.8686\n",
      "Epoch 8/100\n",
      "42750/42750 [==============================] - 18s 415us/step - loss: 0.3556 - acc: 0.8758\n",
      "Epoch 9/100\n",
      "42750/42750 [==============================] - 18s 416us/step - loss: 0.3328 - acc: 0.8847\n",
      "Epoch 10/100\n",
      "42750/42750 [==============================] - 18s 415us/step - loss: 0.3135 - acc: 0.8906\n",
      "Epoch 11/100\n",
      "42750/42750 [==============================] - 18s 416us/step - loss: 0.2941 - acc: 0.8969\n",
      "Epoch 12/100\n",
      "42750/42750 [==============================] - 18s 416us/step - loss: 0.2792 - acc: 0.9001\n",
      "Epoch 13/100\n",
      "42750/42750 [==============================] - 18s 417us/step - loss: 0.2656 - acc: 0.9051\n",
      "Epoch 14/100\n",
      "42750/42750 [==============================] - 18s 413us/step - loss: 0.2541 - acc: 0.9092\n",
      "Epoch 15/100\n",
      "42750/42750 [==============================] - 18s 415us/step - loss: 0.2436 - acc: 0.9125\n",
      "Epoch 16/100\n",
      "42750/42750 [==============================] - 18s 415us/step - loss: 0.2331 - acc: 0.9137\n",
      "Epoch 17/100\n",
      "42750/42750 [==============================] - 18s 416us/step - loss: 0.2288 - acc: 0.9161\n",
      "Epoch 18/100\n",
      "42750/42750 [==============================] - 18s 412us/step - loss: 0.2229 - acc: 0.9180\n",
      "Epoch 19/100\n",
      "42750/42750 [==============================] - 18s 412us/step - loss: 0.2129 - acc: 0.9220\n",
      "Epoch 20/100\n",
      "42750/42750 [==============================] - 18s 412us/step - loss: 0.2039 - acc: 0.9252\n",
      "Epoch 21/100\n",
      "42750/42750 [==============================] - 18s 413us/step - loss: 0.2000 - acc: 0.9265\n",
      "Epoch 22/100\n",
      "42750/42750 [==============================] - 18s 412us/step - loss: 0.2013 - acc: 0.9264\n",
      "Epoch 23/100\n",
      "42750/42750 [==============================] - 17s 407us/step - loss: 0.1915 - acc: 0.9295\n",
      "Epoch 24/100\n",
      "42750/42750 [==============================] - 17s 406us/step - loss: 0.1886 - acc: 0.9312\n",
      "Epoch 25/100\n",
      "42750/42750 [==============================] - 18s 410us/step - loss: 0.1806 - acc: 0.9338\n",
      "Epoch 26/100\n",
      "42750/42750 [==============================] - 18s 410us/step - loss: 0.1762 - acc: 0.9342\n",
      "Epoch 27/100\n",
      "42750/42750 [==============================] - 18s 410us/step - loss: 0.1801 - acc: 0.9341\n",
      "Epoch 28/100\n",
      "42750/42750 [==============================] - 17s 409us/step - loss: 0.1749 - acc: 0.9347\n",
      "Epoch 29/100\n",
      "42750/42750 [==============================] - 18s 410us/step - loss: 0.1762 - acc: 0.9352\n",
      "Epoch 30/100\n",
      "42750/42750 [==============================] - 18s 411us/step - loss: 0.1663 - acc: 0.9385\n",
      "Epoch 31/100\n",
      "42750/42750 [==============================] - 18s 410us/step - loss: 0.1673 - acc: 0.9389\n",
      "Epoch 32/100\n",
      "42750/42750 [==============================] - 17s 409us/step - loss: 0.1669 - acc: 0.9396\n",
      "Epoch 33/100\n",
      "42750/42750 [==============================] - 18s 411us/step - loss: 0.1565 - acc: 0.9423\n",
      "Epoch 34/100\n",
      "42750/42750 [==============================] - 18s 410us/step - loss: 0.1579 - acc: 0.9441\n",
      "Epoch 35/100\n",
      "42750/42750 [==============================] - 18s 411us/step - loss: 0.1628 - acc: 0.9422\n",
      "Epoch 36/100\n",
      "42750/42750 [==============================] - 18s 411us/step - loss: 0.1555 - acc: 0.9417\n",
      "Epoch 37/100\n",
      "42750/42750 [==============================] - 18s 411us/step - loss: 0.1528 - acc: 0.9435\n",
      "Epoch 38/100\n",
      "42750/42750 [==============================] - 18s 411us/step - loss: 0.1517 - acc: 0.9441\n",
      "Epoch 39/100\n",
      "42750/42750 [==============================] - 18s 412us/step - loss: 0.1512 - acc: 0.9449\n",
      "Epoch 40/100\n",
      "42750/42750 [==============================] - 18s 411us/step - loss: 0.1483 - acc: 0.9450\n",
      "Epoch 41/100\n",
      "42750/42750 [==============================] - 18s 410us/step - loss: 0.1518 - acc: 0.9437\n",
      "Epoch 42/100\n",
      "42750/42750 [==============================] - 18s 410us/step - loss: 0.1435 - acc: 0.9473\n",
      "Epoch 43/100\n",
      "42750/42750 [==============================] - 18s 411us/step - loss: 0.1455 - acc: 0.9474\n",
      "Epoch 44/100\n",
      "42750/42750 [==============================] - 18s 411us/step - loss: 0.1407 - acc: 0.9475\n",
      "Epoch 45/100\n",
      "42750/42750 [==============================] - 18s 411us/step - loss: 0.1425 - acc: 0.9477\n",
      "Epoch 46/100\n",
      "42750/42750 [==============================] - 18s 410us/step - loss: 0.1433 - acc: 0.9484\n",
      "Epoch 47/100\n",
      "42750/42750 [==============================] - 18s 411us/step - loss: 0.1389 - acc: 0.9484\n",
      "Epoch 48/100\n",
      "42750/42750 [==============================] - 18s 411us/step - loss: 0.1410 - acc: 0.9493\n",
      "Epoch 49/100\n",
      "42750/42750 [==============================] - 18s 411us/step - loss: 0.1411 - acc: 0.9495\n",
      "Epoch 50/100\n",
      "42750/42750 [==============================] - 18s 411us/step - loss: 0.1320 - acc: 0.9523\n",
      "Epoch 51/100\n",
      "42750/42750 [==============================] - 18s 412us/step - loss: 0.1328 - acc: 0.9516\n",
      "Epoch 52/100\n",
      "42750/42750 [==============================] - 18s 410us/step - loss: 0.1370 - acc: 0.9507\n",
      "Epoch 53/100\n",
      "42750/42750 [==============================] - 18s 411us/step - loss: 0.1367 - acc: 0.9521\n",
      "Epoch 54/100\n",
      "42750/42750 [==============================] - 18s 411us/step - loss: 0.1327 - acc: 0.9523\n",
      "Epoch 55/100\n",
      "42750/42750 [==============================] - 18s 409us/step - loss: 0.1384 - acc: 0.9511\n",
      "Epoch 56/100\n",
      "42750/42750 [==============================] - 18s 410us/step - loss: 0.1316 - acc: 0.9531\n",
      "Epoch 57/100\n",
      "42750/42750 [==============================] - 17s 409us/step - loss: 0.1354 - acc: 0.9523\n",
      "Epoch 58/100\n",
      "42750/42750 [==============================] - 18s 411us/step - loss: 0.1341 - acc: 0.9513\n",
      "Epoch 59/100\n",
      "42750/42750 [==============================] - 18s 410us/step - loss: 0.1310 - acc: 0.9535\n",
      "Epoch 60/100\n",
      "42750/42750 [==============================] - 18s 412us/step - loss: 0.1297 - acc: 0.9535\n",
      "Epoch 61/100\n",
      "42750/42750 [==============================] - 23s 527us/step - loss: 0.1279 - acc: 0.9550\n",
      "Epoch 62/100\n",
      "42750/42750 [==============================] - 18s 433us/step - loss: 0.1263 - acc: 0.9537\n",
      "Epoch 63/100\n",
      "42750/42750 [==============================] - 18s 432us/step - loss: 0.1227 - acc: 0.9564\n",
      "Epoch 64/100\n",
      "42750/42750 [==============================] - 18s 433us/step - loss: 0.1257 - acc: 0.9550\n",
      "Epoch 65/100\n",
      "42750/42750 [==============================] - 19s 434us/step - loss: 0.1265 - acc: 0.9547\n",
      "Epoch 66/100\n",
      "42750/42750 [==============================] - 18s 432us/step - loss: 0.1225 - acc: 0.9559\n",
      "Epoch 67/100\n",
      "42750/42750 [==============================] - 18s 421us/step - loss: 0.1230 - acc: 0.9555\n",
      "Epoch 68/100\n",
      "42750/42750 [==============================] - 17s 408us/step - loss: 0.1276 - acc: 0.9562\n",
      "Epoch 69/100\n",
      "42750/42750 [==============================] - 22s 508us/step - loss: 0.1238 - acc: 0.9549\n",
      "Epoch 70/100\n",
      "42750/42750 [==============================] - 18s 431us/step - loss: 0.1276 - acc: 0.9545\n",
      "Epoch 71/100\n",
      "42750/42750 [==============================] - 18s 430us/step - loss: 0.1193 - acc: 0.9566\n",
      "Epoch 72/100\n",
      "42750/42750 [==============================] - 18s 430us/step - loss: 0.1233 - acc: 0.9566\n",
      "Epoch 73/100\n",
      "42750/42750 [==============================] - 18s 429us/step - loss: 0.1230 - acc: 0.9559\n",
      "Epoch 74/100\n",
      "42750/42750 [==============================] - 18s 430us/step - loss: 0.1182 - acc: 0.9582\n",
      "Epoch 75/100\n",
      "42750/42750 [==============================] - 18s 429us/step - loss: 0.1178 - acc: 0.9571\n",
      "Epoch 76/100\n",
      "42750/42750 [==============================] - 18s 429us/step - loss: 0.1190 - acc: 0.9574\n",
      "Epoch 77/100\n",
      "42750/42750 [==============================] - 18s 431us/step - loss: 0.1184 - acc: 0.9583\n",
      "Epoch 78/100\n",
      "42750/42750 [==============================] - 18s 429us/step - loss: 0.1221 - acc: 0.9574\n",
      "Epoch 79/100\n",
      "42750/42750 [==============================] - 18s 430us/step - loss: 0.1183 - acc: 0.9582\n",
      "Epoch 80/100\n",
      "42750/42750 [==============================] - 18s 432us/step - loss: 0.1221 - acc: 0.9569\n",
      "Epoch 81/100\n",
      "42750/42750 [==============================] - 18s 430us/step - loss: 0.1199 - acc: 0.9569\n",
      "Epoch 82/100\n",
      "42750/42750 [==============================] - 18s 432us/step - loss: 0.1181 - acc: 0.9586\n",
      "Epoch 83/100\n",
      "42750/42750 [==============================] - 18s 428us/step - loss: 0.1224 - acc: 0.9572\n",
      "Epoch 84/100\n",
      "42750/42750 [==============================] - 18s 429us/step - loss: 0.1205 - acc: 0.9565\n",
      "Epoch 85/100\n",
      "42750/42750 [==============================] - 18s 429us/step - loss: 0.1170 - acc: 0.9581\n",
      "Epoch 86/100\n",
      "42750/42750 [==============================] - 18s 429us/step - loss: 0.1202 - acc: 0.9569\n",
      "Epoch 87/100\n",
      "42750/42750 [==============================] - 18s 432us/step - loss: 0.1149 - acc: 0.9604\n",
      "Epoch 88/100\n",
      "42750/42750 [==============================] - 18s 431us/step - loss: 0.1188 - acc: 0.9587\n",
      "Epoch 89/100\n",
      "42750/42750 [==============================] - 18s 431us/step - loss: 0.1179 - acc: 0.9590\n",
      "Epoch 90/100\n",
      "42750/42750 [==============================] - 18s 432us/step - loss: 0.1165 - acc: 0.9580\n",
      "Epoch 91/100\n",
      "42750/42750 [==============================] - 18s 429us/step - loss: 0.1135 - acc: 0.9591\n",
      "Epoch 92/100\n",
      "42750/42750 [==============================] - 18s 431us/step - loss: 0.1180 - acc: 0.9581\n",
      "Epoch 93/100\n",
      "42750/42750 [==============================] - 18s 430us/step - loss: 0.1120 - acc: 0.9596\n",
      "Epoch 94/100\n",
      "42750/42750 [==============================] - 18s 429us/step - loss: 0.1190 - acc: 0.9586\n",
      "Epoch 95/100\n",
      "42750/42750 [==============================] - 18s 428us/step - loss: 0.1157 - acc: 0.9597\n",
      "Epoch 96/100\n",
      "42750/42750 [==============================] - 18s 431us/step - loss: 0.1179 - acc: 0.9590\n",
      "Epoch 97/100\n",
      "42750/42750 [==============================] - 18s 429us/step - loss: 0.1177 - acc: 0.9595\n",
      "Epoch 98/100\n",
      "42750/42750 [==============================] - 18s 431us/step - loss: 0.1140 - acc: 0.9607\n",
      "Epoch 99/100\n",
      "42750/42750 [==============================] - 18s 429us/step - loss: 0.1185 - acc: 0.9599\n",
      "Epoch 100/100\n",
      "42750/42750 [==============================] - 18s 429us/step - loss: 0.1158 - acc: 0.9591\n",
      "\n",
      "Testing acc: 0.8346666667196486, loss: 0.9010280759599474\n",
      "\n",
      "Predicition Probabilities: [2.2520642e-20 3.5908320e-29 1.6240885e-20 2.2171397e-21 1.6764436e-18\n",
      " 1.6025718e-03 9.9839741e-01 1.4012110e-13 2.6808193e-26]\n",
      "\n",
      "Predicted Class: banana\n",
      "Actual Class: banana\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPoAAAD4CAYAAADFJPs2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f946c00b0b8>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "nn.train(100)\n",
    "nn.test()\n",
    "nn.randomPrediction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 401
    },
    "colab_type": "code",
    "id": "uIakfIhPrYgE",
    "outputId": "9c089f45-767d-476f-ed13-d6e6ab1d6aba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing acc: 0.8346666667196486, loss: 0.9010280759599474\n",
      "\n",
      "Predicition Probabilities: [4.6289379e-19 3.0709952e-30 6.9225376e-18 1.0000000e+00 7.0242705e-30\n",
      " 2.8021198e-23 2.2975686e-31 1.9989512e-16 1.2060971e-22]\n",
      "\n",
      "Predicted Class: pineapple\n",
      "Actual Class: pineapple\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPoAAAD4CAYAAADFJPs2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\naar9OwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f946d5b4320>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "nn.test()\n",
    "nn.randomPrediction()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "cnn.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
